{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a3f60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gregmaggard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04992dda",
   "metadata": {},
   "source": [
    "# Acquiring Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d17c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b902b584",
   "metadata": {},
   "source": [
    "# Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0dfc5",
   "metadata": {},
   "source": [
    "## 1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee348017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    string = string.lower()\n",
    "    string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    string = re.sub(r\"[^\\da-z\\'\\s]\", '', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddc5c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am allergic to potatoes it makes me sad'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_clean(\"į åm ällergic to pótatòēs. It makes me sad.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb57ee",
   "metadata": {},
   "source": [
    "## 2. Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910b8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    string = tokenizer.tokenize(string, return_str=True)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a283ab68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I ' m taking my favorite puppy to the grooming salon .\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"I'm taking my favorite puppy to the grooming salon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2884871",
   "metadata": {},
   "source": [
    "## 3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9ca994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    string_stemmed = ' '.join(stems)\n",
    "    return string_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67d769a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'm take my favorit puppi to the groom salon.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(\"I'm taking my favorite puppy to the grooming salon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a4887",
   "metadata": {},
   "source": [
    "## 4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4859b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e3e06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'took', 'my', 'favorite', 'puppy', 'to', 'the', 'grooming', 'salon.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"I took my favorite puppy to the grooming salon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630dcf73",
   "metadata": {},
   "source": [
    "##  5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6103c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    words = string.split()\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "006a36b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We stuff dreams made on.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"We are such stuff as dreams are made on.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cbb58c",
   "metadata": {},
   "source": [
    "## 6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77576c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['business', 'sports', 'technology', 'entertainment']\n",
    "\n",
    "news_df = acquire.get_shorts_articles(categories, refresh = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d297265e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rupee hits 80 per US dollar for the first time...</td>\n",
       "      <td>The Indian rupee touched 80 per US dollar for ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ED arrests ex-Mumbai Police chief Sanjay Pande...</td>\n",
       "      <td>The Enforcement Directorate (ED) on Tuesday ar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are now the world's 10 richest people as A...</td>\n",
       "      <td>Gautam Adani has overtaken Bill Gates to becom...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gautam Adani overtakes Bill Gates to become wo...</td>\n",
       "      <td>Gautam Adani has overtaken Bill Gates to becom...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>List of items exempt from GST when sold loose ...</td>\n",
       "      <td>Amid criticism over pre-packaged and pre-label...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Rupee hits 80 per US dollar for the first time...   \n",
       "1  ED arrests ex-Mumbai Police chief Sanjay Pande...   \n",
       "2  Who are now the world's 10 richest people as A...   \n",
       "3  Gautam Adani overtakes Bill Gates to become wo...   \n",
       "4  List of items exempt from GST when sold loose ...   \n",
       "\n",
       "                                            contents  category  \n",
       "0  The Indian rupee touched 80 per US dollar for ...  business  \n",
       "1  The Enforcement Directorate (ED) on Tuesday ar...  business  \n",
       "2  Gautam Adani has overtaken Bill Gates to becom...  business  \n",
       "3  Gautam Adani has overtaken Bill Gates to becom...  business  \n",
       "4  Amid criticism over pre-packaged and pre-label...  business  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40dc1ee",
   "metadata": {},
   "source": [
    "## 7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df904ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://codeup.com/data-science/jobs-after-a-coding-bootcamp-part-1-data-science/', \n",
    "        'https://codeup.com/featured/what-jobs-can-you-get-after-a-coding-bootcamp-part-2-cloud-administration/',\n",
    "        'https://codeup.com/tips-for-prospective-students/is-our-cloud-administration-program-right-for-you/',\n",
    "        'https://codeup.com/tips-for-prospective-students/mental-health-first-aid-training/',\n",
    "        'https://codeup.com/codeup-news/inclusion-at-codeup-during-pride-month-and-always/']\n",
    "\n",
    "blog_df = acquire.get_blog_articles(urls, refresh = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955d138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cc05591",
   "metadata": {},
   "source": [
    "## 8. For each dataframe, produce the following columns:\n",
    "\n",
    "- title to hold the title\n",
    "- original to hold the original article/post content\n",
    "- clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "- stemmed to hold the stemmed version of the cleaned data.\n",
    "- lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd3920ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    Takes in a DataFrame and a column name in string format, optionally \n",
    "    including lists of extra_words and exclude_words. Returns\n",
    "    a DataFrame with columns listing the title, body of text, \n",
    "    stemmed text, lemmatized text, and the cleaned, tokenized, \n",
    "    and lemmatized text with stopwords removed. \n",
    "    '''\n",
    "    \n",
    "    #Creating a column named 'cleaned' that has the following cleaning functions applied to the text:\n",
    "    df['cleaned'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords,\n",
    "                                  extra_words=extra_words,\n",
    "                                  exclude_words=exclude_words)\n",
    "    \n",
    "    #Creating a separate column that has the stemmed version of the cleaned text:\n",
    "    df['stemmed'] = df['cleaned'].apply(stem)\n",
    "    \n",
    "    #Creating a separate column that has the lemmatized version of the cleaned text:\n",
    "    df['lemmatized'] = df['cleaned'].apply(lemmatize)\n",
    "    \n",
    "    #returning the DataFrame's new columns:\n",
    "    return df[['title', column, 'cleaned', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f96f571f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rupee hits 80 per US dollar for the first time...</td>\n",
       "      <td>The Indian rupee touched 80 per US dollar for ...</td>\n",
       "      <td>indian rupee touched 80 per us dollar first ti...</td>\n",
       "      <td>indian rupe touch 80 per us dollar first time ...</td>\n",
       "      <td>[indian, rupee, touched, 80, per, u, dollar, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ED arrests ex-Mumbai Police chief Sanjay Pande...</td>\n",
       "      <td>The Enforcement Directorate (ED) on Tuesday ar...</td>\n",
       "      <td>enforcement directorate ed tuesday arrested fo...</td>\n",
       "      <td>enforc director ed tuesday arrest former mumba...</td>\n",
       "      <td>[enforcement, directorate, ed, tuesday, arrest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are now the world's 10 richest people as A...</td>\n",
       "      <td>Gautam Adani has overtaken Bill Gates to becom...</td>\n",
       "      <td>gautam adani overtaken bill gates become world...</td>\n",
       "      <td>gautam adani overtaken bill gate becom world '...</td>\n",
       "      <td>[gautam, adani, overtaken, bill, gate, become,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gautam Adani overtakes Bill Gates to become wo...</td>\n",
       "      <td>Gautam Adani has overtaken Bill Gates to becom...</td>\n",
       "      <td>gautam adani overtaken bill gates become world...</td>\n",
       "      <td>gautam adani overtaken bill gate becom world '...</td>\n",
       "      <td>[gautam, adani, overtaken, bill, gate, become,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>List of items exempt from GST when sold loose ...</td>\n",
       "      <td>Amid criticism over pre-packaged and pre-label...</td>\n",
       "      <td>amid criticism prepackaged prelabelled food it...</td>\n",
       "      <td>amid critic prepackag prelabel food item get c...</td>\n",
       "      <td>[amid, criticism, prepackaged, prelabelled, fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Joe Russo arrives in Mumbai ahead of 'The Gray...</td>\n",
       "      <td>Filmmaker Joe Russo has arrived in Mumbai ahea...</td>\n",
       "      <td>filmmaker joe russo arrived mumbai ahead premi...</td>\n",
       "      <td>filmmak joe russo arriv mumbai ahead premier u...</td>\n",
       "      <td>[filmmaker, joe, russo, arrived, mumbai, ahead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I salute Sushmita Sen for living life on her o...</td>\n",
       "      <td>Filmmaker Mahesh Bhatt defended Sushmita Sen a...</td>\n",
       "      <td>filmmaker mahesh bhatt defended sushmita sen t...</td>\n",
       "      <td>filmmak mahesh bhatt defend sushmita sen troll...</td>\n",
       "      <td>[filmmaker, mahesh, bhatt, defended, sushmita,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Fans erect 75-feet banner of 'Liger' actor Vij...</td>\n",
       "      <td>Fans of actor Vijay Deverakonda, who will be s...</td>\n",
       "      <td>fans actor vijay deverakonda seen film ' liger...</td>\n",
       "      <td>fan actor vijay deverakonda seen film ' liger ...</td>\n",
       "      <td>[fan, actor, vijay, deverakonda, seen, film, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Told Salman 'You won't look back after this fi...</td>\n",
       "      <td>Discussing the film 'Partner' where he starred...</td>\n",
       "      <td>discussing film ' partner ' starred actor salm...</td>\n",
       "      <td>discuss film ' partner ' star actor salman kha...</td>\n",
       "      <td>[discussing, film, ', partner, ', starred, act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Yuvraj Singh, Harbhajan approached for 'Jhalak...</td>\n",
       "      <td>Ex-Indian cricketers Yuvraj Singh, Harbhajan S...</td>\n",
       "      <td>exindian cricketers yuvraj singh harbhajan sin...</td>\n",
       "      <td>exindian cricket yuvraj singh harbhajan singh ...</td>\n",
       "      <td>[exindian, cricketer, yuvraj, singh, harbhajan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Rupee hits 80 per US dollar for the first time...   \n",
       "1   ED arrests ex-Mumbai Police chief Sanjay Pande...   \n",
       "2   Who are now the world's 10 richest people as A...   \n",
       "3   Gautam Adani overtakes Bill Gates to become wo...   \n",
       "4   List of items exempt from GST when sold loose ...   \n",
       "..                                                ...   \n",
       "20  Joe Russo arrives in Mumbai ahead of 'The Gray...   \n",
       "21  I salute Sushmita Sen for living life on her o...   \n",
       "22  Fans erect 75-feet banner of 'Liger' actor Vij...   \n",
       "23  Told Salman 'You won't look back after this fi...   \n",
       "24  Yuvraj Singh, Harbhajan approached for 'Jhalak...   \n",
       "\n",
       "                                             contents  \\\n",
       "0   The Indian rupee touched 80 per US dollar for ...   \n",
       "1   The Enforcement Directorate (ED) on Tuesday ar...   \n",
       "2   Gautam Adani has overtaken Bill Gates to becom...   \n",
       "3   Gautam Adani has overtaken Bill Gates to becom...   \n",
       "4   Amid criticism over pre-packaged and pre-label...   \n",
       "..                                                ...   \n",
       "20  Filmmaker Joe Russo has arrived in Mumbai ahea...   \n",
       "21  Filmmaker Mahesh Bhatt defended Sushmita Sen a...   \n",
       "22  Fans of actor Vijay Deverakonda, who will be s...   \n",
       "23  Discussing the film 'Partner' where he starred...   \n",
       "24  Ex-Indian cricketers Yuvraj Singh, Harbhajan S...   \n",
       "\n",
       "                                              cleaned  \\\n",
       "0   indian rupee touched 80 per us dollar first ti...   \n",
       "1   enforcement directorate ed tuesday arrested fo...   \n",
       "2   gautam adani overtaken bill gates become world...   \n",
       "3   gautam adani overtaken bill gates become world...   \n",
       "4   amid criticism prepackaged prelabelled food it...   \n",
       "..                                                ...   \n",
       "20  filmmaker joe russo arrived mumbai ahead premi...   \n",
       "21  filmmaker mahesh bhatt defended sushmita sen t...   \n",
       "22  fans actor vijay deverakonda seen film ' liger...   \n",
       "23  discussing film ' partner ' starred actor salm...   \n",
       "24  exindian cricketers yuvraj singh harbhajan sin...   \n",
       "\n",
       "                                              stemmed  \\\n",
       "0   indian rupe touch 80 per us dollar first time ...   \n",
       "1   enforc director ed tuesday arrest former mumba...   \n",
       "2   gautam adani overtaken bill gate becom world '...   \n",
       "3   gautam adani overtaken bill gate becom world '...   \n",
       "4   amid critic prepackag prelabel food item get c...   \n",
       "..                                                ...   \n",
       "20  filmmak joe russo arriv mumbai ahead premier u...   \n",
       "21  filmmak mahesh bhatt defend sushmita sen troll...   \n",
       "22  fan actor vijay deverakonda seen film ' liger ...   \n",
       "23  discuss film ' partner ' star actor salman kha...   \n",
       "24  exindian cricket yuvraj singh harbhajan singh ...   \n",
       "\n",
       "                                           lemmatized  \n",
       "0   [indian, rupee, touched, 80, per, u, dollar, f...  \n",
       "1   [enforcement, directorate, ed, tuesday, arrest...  \n",
       "2   [gautam, adani, overtaken, bill, gate, become,...  \n",
       "3   [gautam, adani, overtaken, bill, gate, become,...  \n",
       "4   [amid, criticism, prepackaged, prelabelled, fo...  \n",
       "..                                                ...  \n",
       "20  [filmmaker, joe, russo, arrived, mumbai, ahead...  \n",
       "21  [filmmaker, mahesh, bhatt, defended, sushmita,...  \n",
       "22  [fan, actor, vijay, deverakonda, seen, film, '...  \n",
       "23  [discussing, film, ', partner, ', starred, act...  \n",
       "24  [exindian, cricketer, yuvraj, singh, harbhajan...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_article_data(news_df, 'contents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9a0052",
   "metadata": {},
   "source": [
    "## 9. Ask yourself:\n",
    "\n",
    "- If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "- If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32137bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
